{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianshin12/zeta/blob/master/Chapter7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twiJKXGhBp_b",
        "colab_type": "code",
        "outputId": "f11d98b4-5485-4253-8a29-e8e4572205eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DF5UET-B3xt",
        "colab_type": "code",
        "outputId": "993972ad-c165-4ece-c815-a390961b6e31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyxHe7N2B3ih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, os\n",
        "sys.path.insert(0, '/content/drive/My Drive/Colab Notebooks/dataset')\n",
        "import mnist\n",
        "from mnist import load_mnist \n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "from collections import OrderedDict\n",
        "sys.path.insert(0, '/content/drive/My Drive/Colab Notebooks/common')\n",
        "from functions import softmax, cross_entropy_error\n",
        "from gradient import numerical_gradient\n",
        "def sigmoid(x):\n",
        "  return 1 / (1+np.exp(-x))\n",
        "def Relu(x):\n",
        "  return np.maximum(0, x)\n",
        "sys.path.insert(0, '/content/drive/My Drive/Colab Notebooks/')\n",
        "from common.util import im2col\n",
        "from common.trainer import Trainer\n",
        "import pickle\n",
        "from common.layers import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m2qR_uAGZ77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Convolution:\n",
        "  def __init__(self, W, b, stride=1, pad=0): #인수 초기화\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.stride = stride\n",
        "    self.pad = pad\n",
        "  \n",
        "  def forward(self, x):\n",
        "    FN, C, FH, FW = self.W.shape #필터\n",
        "    N, C, H, W = x.shape #입력데이터\n",
        "    out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
        "    out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "    col = im2col(x, FH, FW, self.stride, self.pad) #입력 데이터를 im2col으로 전개\n",
        "    col_W = self.W.reshape(FN, -1).T #필터를 reshape 사용하여 2차원 배열로 전개\n",
        "    out = np.dot(col, col_W) + self.b #전개한 두 행렬의 곱 + 편차\n",
        "\n",
        "    out = out.reshape(N, out_h, out_w, -1).transpose(0,3,1,2) #transpose를 사용하여 출력데이터의 축 순서 변경\n",
        "\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "licpFRpLGZtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Pooling:\n",
        "  def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "    self.pool_h = pool_h\n",
        "    self.pool_w = pool_w\n",
        "    self.stride = stride\n",
        "    self.pad = pad\n",
        "\n",
        "  def forward(self, x):\n",
        "    N, C, H, W = x.shape\n",
        "    out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "    out_w = int(1 + (W - self.pool.w) / self.stride)\n",
        "\n",
        "    #전개\n",
        "    col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "    col = col.reshape(-1, self.pool_h * self.pool_w)\n",
        "\n",
        "    #최댓값\n",
        "    out = np.max(col, axis=1) #첫번째 차원의 축마다 최댓값 계산\n",
        "\n",
        "    #성형\n",
        "    out = out.reshape(N, out_h, out_w, C).transpose(0,3,1,2)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzMOJ78T7zsq",
        "colab_type": "text"
      },
      "source": [
        "## 7.5 CNN구현하기  \n",
        "합성곱 계층과 풀링 계층을 조합하여 손글씨 숫자를 인식하는 CNN을 구현해보겠습니다.  \n",
        "* Input $\\rightarrow$ Conv(합성곱 계층) $\\rightarrow$ ReLU $\\rightarrow$ Pooling(풀링 계층) $\\rightarrow$ Affine $\\rightarrow$ ReLU $\\rightarrow$ Affine $\\rightarrow$ Softmax  \n",
        "\n",
        "\n",
        "초기화(\\_\\_init__) 때 받는 인수  \n",
        "* input_dim : 입력 데이터(채널 수, 높이, 너비)의 차원  \n",
        "* conv_param : 합성곱 계층의 하이퍼파라미터 (딕셔너리 형태)  \n",
        "    1. filter_num : 필터 수  \n",
        "    2. filter_size : 필터 크기  \n",
        "    3. stride : 스트라이드  \n",
        "    4. pad : 패딩  \n",
        "* higgen_size : 은닉층(완전연결)의 뉴런 수  \n",
        "* output_size : 출력층(완전연결)의 뉴런 수  \n",
        "* weight_init_std : 초기화 때의 가중치 표준편차  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7kk-Jt1XTAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleConvNet:\n",
        "    \"\"\"단순한 합성곱 신경망\n",
        "    \n",
        "    conv - relu - pool - affine - relu - affine - softmax\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
        "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
        "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
        "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
        "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
        "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
        "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28), \n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        # 쓰기 쉽도록 딕셔너리 값을 꺼내어 변수 지정\n",
        "\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "        # 합성곱 계층의 출력 크기 계산\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        # 1번째 층의 합성곱 계층의 가중치 및 편향\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        # 2번째 층의 완전연결 계층의 가중치 및 편향\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "                            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "        # 3번째 층의 완전연결 계층의 가중치 및 편향\n",
        "\n",
        "        # CNN을 구성하는 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "# 추론을 수행하는 predict / 손실 함수의 값을 구하는 loss\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"손실 함수를 구한다.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "        acc = 0.0\n",
        "        \n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt) \n",
        "        \n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다（수치미분）.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다(오차역전파법).\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "        \n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q74nOUYtFvhI",
        "colab_type": "code",
        "outputId": "df7c2c01-979c-429a-a91f-aeae3a2c92ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28), conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "                        \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# 매개변수 보존\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss:2.3000268423331116\n",
            "=== epoch:1, train acc:0.241, test acc:0.189 ===\n",
            "train loss:2.297303897749794\n",
            "train loss:2.2949004471887533\n",
            "train loss:2.2873106980097844\n",
            "train loss:2.2810838883365\n",
            "train loss:2.273402408168741\n",
            "train loss:2.2499615047407024\n",
            "train loss:2.2363226334352917\n",
            "train loss:2.226550646671099\n",
            "train loss:2.2059696556298953\n",
            "train loss:2.1538767675116417\n",
            "train loss:2.123663548106514\n",
            "train loss:2.055195213705233\n",
            "train loss:2.053979247161511\n",
            "train loss:2.000216440119104\n",
            "train loss:1.925907981978532\n",
            "train loss:1.8385495091741957\n",
            "train loss:1.7952901639115262\n",
            "train loss:1.6764863500349265\n",
            "train loss:1.65931664291933\n",
            "train loss:1.4294292658888548\n",
            "train loss:1.492046873615424\n",
            "train loss:1.3388718499768066\n",
            "train loss:1.409670974347756\n",
            "train loss:1.1932042011147426\n",
            "train loss:1.1397121867179043\n",
            "train loss:1.0500461026071963\n",
            "train loss:1.003745363151504\n",
            "train loss:0.8962046754275624\n",
            "train loss:0.9373284859366688\n",
            "train loss:0.8438700295670999\n",
            "train loss:0.8141210981470397\n",
            "train loss:0.7140456354064684\n",
            "train loss:0.7979816869982059\n",
            "train loss:0.7131467615220234\n",
            "train loss:0.6549611578375594\n",
            "train loss:0.650251533257469\n",
            "train loss:0.771945154596558\n",
            "train loss:0.687918634393761\n",
            "train loss:0.5879882157744741\n",
            "train loss:0.6630717281980515\n",
            "train loss:0.4321581613726903\n",
            "train loss:0.6746156390595623\n",
            "train loss:0.48698707488615606\n",
            "train loss:0.7833540656230125\n",
            "train loss:0.9422289688584606\n",
            "train loss:0.45667178416767734\n",
            "train loss:0.5881353812507002\n",
            "train loss:0.567022560000891\n",
            "train loss:0.4459398574630138\n",
            "train loss:0.42345096966110474\n",
            "=== epoch:2, train acc:0.804, test acc:0.783 ===\n",
            "train loss:0.510950157153762\n",
            "train loss:0.6215401443179714\n",
            "train loss:0.7717478178448242\n",
            "train loss:0.39582547959326697\n",
            "train loss:0.42684222105272185\n",
            "train loss:0.3872173982927054\n",
            "train loss:0.6030785241322316\n",
            "train loss:0.5740930805082125\n",
            "train loss:0.446051978989948\n",
            "train loss:0.6198378592413798\n",
            "train loss:0.47707920510387986\n",
            "train loss:0.43671500880624764\n",
            "train loss:0.43436400138245207\n",
            "train loss:0.5507997611584454\n",
            "train loss:0.32624956470306143\n",
            "train loss:0.29313894453675876\n",
            "train loss:0.36373084542469825\n",
            "train loss:0.37076691186043786\n",
            "train loss:0.34431060029685395\n",
            "train loss:0.47598298553829665\n",
            "train loss:0.4261941949792219\n",
            "train loss:0.37184353497681366\n",
            "train loss:0.44451162231898933\n",
            "train loss:0.39674160147516085\n",
            "train loss:0.3177225734496736\n",
            "train loss:0.3437289473980031\n",
            "train loss:0.41399468083462776\n",
            "train loss:0.38046156463780273\n",
            "train loss:0.3995381371233536\n",
            "train loss:0.29698503711671814\n",
            "train loss:0.37879830224622546\n",
            "train loss:0.27116993125887234\n",
            "train loss:0.5530712824500046\n",
            "train loss:0.6126581634306687\n",
            "train loss:0.22966957553869916\n",
            "train loss:0.4517060067363986\n",
            "train loss:0.537940390590337\n",
            "train loss:0.27108103564881975\n",
            "train loss:0.2780896540208177\n",
            "train loss:0.4165302955510203\n",
            "train loss:0.3227603462380431\n",
            "train loss:0.3102978904701824\n",
            "train loss:0.24987670402980836\n",
            "train loss:0.44124816702405956\n",
            "train loss:0.18469511474318917\n",
            "train loss:0.23342832053157772\n",
            "train loss:0.3721943534496599\n",
            "train loss:0.37466566878524526\n",
            "train loss:0.2715839517828399\n",
            "train loss:0.25567953675242555\n",
            "=== epoch:3, train acc:0.873, test acc:0.873 ===\n",
            "train loss:0.4697244926418461\n",
            "train loss:0.1686581594445152\n",
            "train loss:0.45528462412223264\n",
            "train loss:0.3166516153045531\n",
            "train loss:0.33596040676249744\n",
            "train loss:0.28433270878095324\n",
            "train loss:0.39663813181935514\n",
            "train loss:0.3569931501283983\n",
            "train loss:0.3838909217941434\n",
            "train loss:0.29644535058192195\n",
            "train loss:0.3325033909437842\n",
            "train loss:0.20699031819559327\n",
            "train loss:0.43901881335741094\n",
            "train loss:0.40719431805057094\n",
            "train loss:0.44533275856264126\n",
            "train loss:0.29914577607452963\n",
            "train loss:0.4750084065799383\n",
            "train loss:0.29698841424780137\n",
            "train loss:0.27316719466345435\n",
            "train loss:0.26839853606706915\n",
            "train loss:0.3738885924064772\n",
            "train loss:0.3498422178581306\n",
            "train loss:0.5484195056397675\n",
            "train loss:0.2978940587520703\n",
            "train loss:0.363469095986075\n",
            "train loss:0.3081795125849609\n",
            "train loss:0.2570624650521791\n",
            "train loss:0.29972501336359164\n",
            "train loss:0.502815239426769\n",
            "train loss:0.27384994174994487\n",
            "train loss:0.2232992330529029\n",
            "train loss:0.1384881390417341\n",
            "train loss:0.31739944233297523\n",
            "train loss:0.2814721440128983\n",
            "train loss:0.33954163793797937\n",
            "train loss:0.24336952583818772\n",
            "train loss:0.2869288680336797\n",
            "train loss:0.2832975108528439\n",
            "train loss:0.2735903574479055\n",
            "train loss:0.23002634420914\n",
            "train loss:0.49627999052275784\n",
            "train loss:0.3144351809634988\n",
            "train loss:0.3479803332794505\n",
            "train loss:0.3716557370909669\n",
            "train loss:0.39431226433503463\n",
            "train loss:0.26112302180715763\n",
            "train loss:0.3077918183787689\n",
            "train loss:0.4244528653476336\n",
            "train loss:0.3783222299336116\n",
            "train loss:0.33502591005675747\n",
            "=== epoch:4, train acc:0.886, test acc:0.869 ===\n",
            "train loss:0.35565470616343986\n",
            "train loss:0.20680111116341862\n",
            "train loss:0.23211281247945265\n",
            "train loss:0.22646694240396806\n",
            "train loss:0.3623636384934715\n",
            "train loss:0.3017719159437502\n",
            "train loss:0.2719663254382146\n",
            "train loss:0.16037043145237542\n",
            "train loss:0.2502342063225961\n",
            "train loss:0.201388916923748\n",
            "train loss:0.26382346413184554\n",
            "train loss:0.34364095524886096\n",
            "train loss:0.3150181903835052\n",
            "train loss:0.307952968863562\n",
            "train loss:0.2712545037841855\n",
            "train loss:0.32834904463534825\n",
            "train loss:0.20840442947403442\n",
            "train loss:0.28910184702577063\n",
            "train loss:0.22965535142826504\n",
            "train loss:0.35459558853591844\n",
            "train loss:0.49057472575417016\n",
            "train loss:0.3010878642710882\n",
            "train loss:0.21314759596736269\n",
            "train loss:0.15022838122854812\n",
            "train loss:0.32942731403306025\n",
            "train loss:0.2366425252468014\n",
            "train loss:0.2832083835391552\n",
            "train loss:0.1794503495625509\n",
            "train loss:0.2211228389718951\n",
            "train loss:0.22539681433671124\n",
            "train loss:0.314646180211447\n",
            "train loss:0.2829496719444258\n",
            "train loss:0.1528709998850233\n",
            "train loss:0.25338011983976577\n",
            "train loss:0.3328782838131124\n",
            "train loss:0.19412352299141308\n",
            "train loss:0.1993514960062798\n",
            "train loss:0.4176971626834138\n",
            "train loss:0.21043190096007616\n",
            "train loss:0.2155742900886254\n",
            "train loss:0.43857649920842007\n",
            "train loss:0.20855965171140423\n",
            "train loss:0.41541618631863164\n",
            "train loss:0.3348024517257119\n",
            "train loss:0.3638814175095571\n",
            "train loss:0.17981739072345237\n",
            "train loss:0.2742088516099971\n",
            "train loss:0.16381627040612554\n",
            "train loss:0.24652578960513685\n",
            "train loss:0.2892780422515709\n",
            "=== epoch:5, train acc:0.91, test acc:0.901 ===\n",
            "train loss:0.19540206044305472\n",
            "train loss:0.27731337942344875\n",
            "train loss:0.2627482156802838\n",
            "train loss:0.22065651221008564\n",
            "train loss:0.25831814306509293\n",
            "train loss:0.2655105678818224\n",
            "train loss:0.2645405677464749\n",
            "train loss:0.1541741640010359\n",
            "train loss:0.14001726153865995\n",
            "train loss:0.23146436197034903\n",
            "train loss:0.11326935556554153\n",
            "train loss:0.32661773627365276\n",
            "train loss:0.19222521109805782\n",
            "train loss:0.2139378326807434\n",
            "train loss:0.26606594652700244\n",
            "train loss:0.16900145750902087\n",
            "train loss:0.302674414111271\n",
            "train loss:0.1962290723357851\n",
            "train loss:0.14848509587906042\n",
            "train loss:0.27343442321089356\n",
            "train loss:0.19399632199392539\n",
            "train loss:0.09187361895959141\n",
            "train loss:0.23712098109190025\n",
            "train loss:0.1724857116696728\n",
            "train loss:0.1297981103389958\n",
            "train loss:0.23981719590358147\n",
            "train loss:0.1297152957920483\n",
            "train loss:0.18858953961013672\n",
            "train loss:0.18910975398854718\n",
            "train loss:0.16268029132669531\n",
            "train loss:0.27558008583750576\n",
            "train loss:0.2002936972131566\n",
            "train loss:0.2042744231760108\n",
            "train loss:0.17199152747436805\n",
            "train loss:0.24607284574314964\n",
            "train loss:0.36649179737483933\n",
            "train loss:0.10462152892395445\n",
            "train loss:0.16346528735932928\n",
            "train loss:0.24405418703726706\n",
            "train loss:0.21240240919471112\n",
            "train loss:0.1742533437503728\n",
            "train loss:0.09629658445633814\n",
            "train loss:0.17081195344094607\n",
            "train loss:0.13659623582293764\n",
            "train loss:0.2539700545976071\n",
            "train loss:0.22311174942505588\n",
            "train loss:0.3068118428914825\n",
            "train loss:0.22783355281372547\n",
            "train loss:0.10795533323175319\n",
            "train loss:0.20262753139331874\n",
            "=== epoch:6, train acc:0.92, test acc:0.917 ===\n",
            "train loss:0.3749880053482402\n",
            "train loss:0.1609990432549395\n",
            "train loss:0.27673801132604103\n",
            "train loss:0.16939828384477718\n",
            "train loss:0.1713852221962993\n",
            "train loss:0.1004103058278006\n",
            "train loss:0.17591010006346777\n",
            "train loss:0.12226745395266006\n",
            "train loss:0.17037227288930898\n",
            "train loss:0.4185148325670238\n",
            "train loss:0.056965459725238754\n",
            "train loss:0.2695307766414785\n",
            "train loss:0.18304183200364196\n",
            "train loss:0.2618800556461998\n",
            "train loss:0.1915215107835593\n",
            "train loss:0.24599452799837565\n",
            "train loss:0.2959262399012255\n",
            "train loss:0.10006591625837778\n",
            "train loss:0.2695555008053002\n",
            "train loss:0.1435275934449756\n",
            "train loss:0.1590980306198331\n",
            "train loss:0.18105915508918405\n",
            "train loss:0.1617582014405598\n",
            "train loss:0.2901663950379389\n",
            "train loss:0.26397900692920667\n",
            "train loss:0.23466703959758736\n",
            "train loss:0.23740044809879077\n",
            "train loss:0.1897857179709212\n",
            "train loss:0.10648691135657345\n",
            "train loss:0.22256332074225502\n",
            "train loss:0.1117548682332897\n",
            "train loss:0.10304080840732396\n",
            "train loss:0.17061420483916576\n",
            "train loss:0.10730805199245401\n",
            "train loss:0.2116773153082876\n",
            "train loss:0.10447522979106702\n",
            "train loss:0.3204783105534121\n",
            "train loss:0.20544137064469814\n",
            "train loss:0.129348089996297\n",
            "train loss:0.0663771556306244\n",
            "train loss:0.13033931576445265\n",
            "train loss:0.14482348784904475\n",
            "train loss:0.13728114476874007\n",
            "train loss:0.16405309347592995\n",
            "train loss:0.20612520137736037\n",
            "train loss:0.16032773753199003\n",
            "train loss:0.15057140228212926\n",
            "train loss:0.11015881279539524\n",
            "train loss:0.2294137340261768\n",
            "train loss:0.15361934230210894\n",
            "=== epoch:7, train acc:0.932, test acc:0.91 ===\n",
            "train loss:0.11537593877743876\n",
            "train loss:0.19921228431534854\n",
            "train loss:0.05944790910607909\n",
            "train loss:0.1434938132669621\n",
            "train loss:0.1148185251591024\n",
            "train loss:0.13544223281869952\n",
            "train loss:0.19520689505702285\n",
            "train loss:0.18079572256035245\n",
            "train loss:0.1618133935627034\n",
            "train loss:0.30954792462029024\n",
            "train loss:0.16596271802460938\n",
            "train loss:0.1638790178221949\n",
            "train loss:0.13881145115531113\n",
            "train loss:0.13703486644611226\n",
            "train loss:0.16228510017868672\n",
            "train loss:0.10671673934555576\n",
            "train loss:0.19118187742748788\n",
            "train loss:0.09284703124757049\n",
            "train loss:0.11139472784963851\n",
            "train loss:0.15057429775528663\n",
            "train loss:0.06469065734468557\n",
            "train loss:0.15149227499405926\n",
            "train loss:0.23879445808656116\n",
            "train loss:0.14032403886223396\n",
            "train loss:0.18870224585791331\n",
            "train loss:0.31725225900523635\n",
            "train loss:0.10394378487660816\n",
            "train loss:0.2528430548432372\n",
            "train loss:0.1294973231662003\n",
            "train loss:0.13454469612122255\n",
            "train loss:0.25218347518888673\n",
            "train loss:0.18532455515759672\n",
            "train loss:0.16267262110270983\n",
            "train loss:0.15088615686941462\n",
            "train loss:0.12556955106525086\n",
            "train loss:0.1184790011108873\n",
            "train loss:0.2867256665789132\n",
            "train loss:0.22136442273561893\n",
            "train loss:0.14257745018616771\n",
            "train loss:0.09564356557410969\n",
            "train loss:0.22023351478140799\n",
            "train loss:0.14786624570710508\n",
            "train loss:0.13717304459124807\n",
            "train loss:0.09703373098943281\n",
            "train loss:0.09357675541346089\n",
            "train loss:0.12207870880622085\n",
            "train loss:0.2064254625629877\n",
            "train loss:0.1581201051769\n",
            "train loss:0.08492584619791897\n",
            "train loss:0.12016938632050529\n",
            "=== epoch:8, train acc:0.949, test acc:0.928 ===\n",
            "train loss:0.1234404245626374\n",
            "train loss:0.10819846183970876\n",
            "train loss:0.1417999736417816\n",
            "train loss:0.2827273030570698\n",
            "train loss:0.16009583503742333\n",
            "train loss:0.10709635467112878\n",
            "train loss:0.10263709512224752\n",
            "train loss:0.22972015246576916\n",
            "train loss:0.2938980303047517\n",
            "train loss:0.1364253948589872\n",
            "train loss:0.21378105655477356\n",
            "train loss:0.162556693243057\n",
            "train loss:0.21881274388176433\n",
            "train loss:0.11506572046403342\n",
            "train loss:0.159771324139768\n",
            "train loss:0.1368332268410267\n",
            "train loss:0.23679242450574217\n",
            "train loss:0.08949729029724363\n",
            "train loss:0.09682440943317056\n",
            "train loss:0.1172128952781301\n",
            "train loss:0.0918383714388042\n",
            "train loss:0.1355823646407652\n",
            "train loss:0.14508796049887007\n",
            "train loss:0.24610583201236658\n",
            "train loss:0.08034014230898757\n",
            "train loss:0.13786764085413736\n",
            "train loss:0.11793901710795279\n",
            "train loss:0.12522956506504715\n",
            "train loss:0.12119937348692947\n",
            "train loss:0.12308362224913581\n",
            "train loss:0.12994466036919547\n",
            "train loss:0.1648971507544956\n",
            "train loss:0.2493497896872282\n",
            "train loss:0.126855665479569\n",
            "train loss:0.15181583226629358\n",
            "train loss:0.0928054679102747\n",
            "train loss:0.11517966751527205\n",
            "train loss:0.1766710791024591\n",
            "train loss:0.141651035299734\n",
            "train loss:0.09120505269395361\n",
            "train loss:0.09793038940339104\n",
            "train loss:0.10679610953358035\n",
            "train loss:0.11044688649956093\n",
            "train loss:0.06700036996997834\n",
            "train loss:0.05627059886047978\n",
            "train loss:0.14294621341434147\n",
            "train loss:0.1860250302612687\n",
            "train loss:0.10091034627624668\n",
            "train loss:0.16310992791476792\n",
            "train loss:0.2207921035053501\n",
            "=== epoch:9, train acc:0.942, test acc:0.917 ===\n",
            "train loss:0.1115375504343391\n",
            "train loss:0.13984316733586333\n",
            "train loss:0.18826658527249482\n",
            "train loss:0.22659690010825229\n",
            "train loss:0.10569085834123626\n",
            "train loss:0.08258541606040398\n",
            "train loss:0.1269679027940781\n",
            "train loss:0.12237131271697071\n",
            "train loss:0.27737468929960246\n",
            "train loss:0.08396138108691588\n",
            "train loss:0.13459588209972823\n",
            "train loss:0.09315445732782632\n",
            "train loss:0.11910179102447367\n",
            "train loss:0.08558431796004028\n",
            "train loss:0.21709431727792766\n",
            "train loss:0.13772569929091627\n",
            "train loss:0.1336257572346404\n",
            "train loss:0.06507781308484267\n",
            "train loss:0.13205734760440105\n",
            "train loss:0.16310291346407793\n",
            "train loss:0.11979358813559758\n",
            "train loss:0.12823656646027232\n",
            "train loss:0.10367695762662824\n",
            "train loss:0.13707419085178268\n",
            "train loss:0.04900433651768698\n",
            "train loss:0.06759859338891419\n",
            "train loss:0.06403912245588213\n",
            "train loss:0.19162986959944306\n",
            "train loss:0.18313679528846444\n",
            "train loss:0.12691284909339362\n",
            "train loss:0.13482213234082926\n",
            "train loss:0.1112853421343518\n",
            "train loss:0.18144380705173047\n",
            "train loss:0.11778789196217593\n",
            "train loss:0.15117002920570913\n",
            "train loss:0.1506931690630552\n",
            "train loss:0.0433504960557686\n",
            "train loss:0.08672342566431687\n",
            "train loss:0.13325512869351577\n",
            "train loss:0.1665508700886608\n",
            "train loss:0.16971244821447568\n",
            "train loss:0.16373141404178987\n",
            "train loss:0.23399348067626352\n",
            "train loss:0.1337219865324777\n",
            "train loss:0.15661666464362578\n",
            "train loss:0.14790741782654587\n",
            "train loss:0.1374936621451358\n",
            "train loss:0.21185560134338702\n",
            "train loss:0.15451532365880152\n",
            "train loss:0.050095281553875194\n",
            "=== epoch:10, train acc:0.965, test acc:0.936 ===\n",
            "train loss:0.11967327587526366\n",
            "train loss:0.04817835909747617\n",
            "train loss:0.09830551900748966\n",
            "train loss:0.0833330685651635\n",
            "train loss:0.17230262345998373\n",
            "train loss:0.0958404426308188\n",
            "train loss:0.05262898508494753\n",
            "train loss:0.0812343771847791\n",
            "train loss:0.21007108349267553\n",
            "train loss:0.11152603839144289\n",
            "train loss:0.06211762198139069\n",
            "train loss:0.06264307373646003\n",
            "train loss:0.21234683242520994\n",
            "train loss:0.07945067503008298\n",
            "train loss:0.14576939385935395\n",
            "train loss:0.09349253898460087\n",
            "train loss:0.08363808763868902\n",
            "train loss:0.06828464470643734\n",
            "train loss:0.12563414949134627\n",
            "train loss:0.09560802164707603\n",
            "train loss:0.13616714934797564\n",
            "train loss:0.05657661629110113\n",
            "train loss:0.08993978769589925\n",
            "train loss:0.22605601062263708\n",
            "train loss:0.10718419504242982\n",
            "train loss:0.10800932387120107\n",
            "train loss:0.07368099793788235\n",
            "train loss:0.06261141217384524\n",
            "train loss:0.092554864941765\n",
            "train loss:0.09321347055906092\n",
            "train loss:0.0835550361981939\n",
            "train loss:0.07574407161513688\n",
            "train loss:0.07887302315087098\n",
            "train loss:0.13373181537074802\n",
            "train loss:0.13865705345026938\n",
            "train loss:0.14009047669020172\n",
            "train loss:0.07770641471010178\n",
            "train loss:0.085923645006472\n",
            "train loss:0.1712040184365484\n",
            "train loss:0.06272048201796421\n",
            "train loss:0.0522504434901874\n",
            "train loss:0.15174949500148793\n",
            "train loss:0.0943374241073473\n",
            "train loss:0.13648510603466144\n",
            "train loss:0.06292994124411923\n",
            "train loss:0.12980455427808443\n",
            "train loss:0.17047004027979326\n",
            "train loss:0.1317523235186879\n",
            "train loss:0.08980249295819791\n",
            "train loss:0.0846112156506525\n",
            "=== epoch:11, train acc:0.963, test acc:0.941 ===\n",
            "train loss:0.11022848753275995\n",
            "train loss:0.0894328186866522\n",
            "train loss:0.06687924372797749\n",
            "train loss:0.17137180496277754\n",
            "train loss:0.12010889150427935\n",
            "train loss:0.14407975854064603\n",
            "train loss:0.16542318924983307\n",
            "train loss:0.11168685953781296\n",
            "train loss:0.0424962472114912\n",
            "train loss:0.0885611969391563\n",
            "train loss:0.05291310248122006\n",
            "train loss:0.0952836226490156\n",
            "train loss:0.07443448796714533\n",
            "train loss:0.12175243158575992\n",
            "train loss:0.1106101129938549\n",
            "train loss:0.1043512252551376\n",
            "train loss:0.07685428670094421\n",
            "train loss:0.060584613332455683\n",
            "train loss:0.09252322375364776\n",
            "train loss:0.05346441702347818\n",
            "train loss:0.08739305424553862\n",
            "train loss:0.049159719834845746\n",
            "train loss:0.09756951208365267\n",
            "train loss:0.18526601891055003\n",
            "train loss:0.080251761778006\n",
            "train loss:0.139595336876482\n",
            "train loss:0.2131385952155406\n",
            "train loss:0.16611104463879336\n",
            "train loss:0.15177680810405017\n",
            "train loss:0.07975174707509365\n",
            "train loss:0.09568565095829358\n",
            "train loss:0.06654361266603641\n",
            "train loss:0.055975653523855834\n",
            "train loss:0.19149609133466464\n",
            "train loss:0.09441470046609361\n",
            "train loss:0.08089298210104433\n",
            "train loss:0.09427987442876923\n",
            "train loss:0.0910570510764005\n",
            "train loss:0.09216595393002561\n",
            "train loss:0.14697856200495008\n",
            "train loss:0.17801519553088851\n",
            "train loss:0.07983827718965078\n",
            "train loss:0.0998016494731777\n",
            "train loss:0.06437258542859449\n",
            "train loss:0.09241847845357125\n",
            "train loss:0.20183381392728392\n",
            "train loss:0.06598822590495511\n",
            "train loss:0.08100888208882565\n",
            "train loss:0.06906275317742791\n",
            "train loss:0.05381546800472461\n",
            "=== epoch:12, train acc:0.962, test acc:0.948 ===\n",
            "train loss:0.120486499553953\n",
            "train loss:0.07632036776189305\n",
            "train loss:0.12029090582440174\n",
            "train loss:0.0987542675374832\n",
            "train loss:0.03777947682289917\n",
            "train loss:0.05032354153991724\n",
            "train loss:0.08966869775444974\n",
            "train loss:0.07385253469394432\n",
            "train loss:0.08065890192729697\n",
            "train loss:0.10730168472037255\n",
            "train loss:0.1525857648420548\n",
            "train loss:0.1337966853407847\n",
            "train loss:0.07097923479177262\n",
            "train loss:0.06749926303396862\n",
            "train loss:0.06589461293996822\n",
            "train loss:0.08179965700492803\n",
            "train loss:0.060903845436567516\n",
            "train loss:0.06978464941289907\n",
            "train loss:0.03556375946867367\n",
            "train loss:0.11481450215418132\n",
            "train loss:0.16331797259690994\n",
            "train loss:0.039703229512912455\n",
            "train loss:0.07799716839871412\n",
            "train loss:0.07026727061364343\n",
            "train loss:0.13276363540675912\n",
            "train loss:0.05409179178963341\n",
            "train loss:0.2848667065157797\n",
            "train loss:0.11521211388044834\n",
            "train loss:0.0924284699862132\n",
            "train loss:0.10601178847127705\n",
            "train loss:0.05868485670586863\n",
            "train loss:0.06814985183161754\n",
            "train loss:0.07200179500067598\n",
            "train loss:0.05336066114158816\n",
            "train loss:0.08838501300524454\n",
            "train loss:0.12205863271148769\n",
            "train loss:0.15294751831804157\n",
            "train loss:0.050073875243888136\n",
            "train loss:0.06882946271516958\n",
            "train loss:0.12348836200843781\n",
            "train loss:0.05821397368293504\n",
            "train loss:0.12393084183061802\n",
            "train loss:0.04336853689042413\n",
            "train loss:0.10051350294713829\n",
            "train loss:0.04942012287998573\n",
            "train loss:0.048588859905166426\n",
            "train loss:0.10321811762530424\n",
            "train loss:0.025537514267192866\n",
            "train loss:0.06984694400169904\n",
            "train loss:0.14405320576330916\n",
            "=== epoch:13, train acc:0.97, test acc:0.947 ===\n",
            "train loss:0.07177109144129509\n",
            "train loss:0.042836679985374826\n",
            "train loss:0.12829237409506747\n",
            "train loss:0.04283353077333811\n",
            "train loss:0.031645337081165494\n",
            "train loss:0.03781089397154842\n",
            "train loss:0.14381196340823646\n",
            "train loss:0.15858684015748162\n",
            "train loss:0.05069364280398754\n",
            "train loss:0.08323285310146865\n",
            "train loss:0.07242113283681985\n",
            "train loss:0.10430252677723909\n",
            "train loss:0.10340692543252825\n",
            "train loss:0.08026047709325396\n",
            "train loss:0.07172680314000815\n",
            "train loss:0.11274062885063316\n",
            "train loss:0.10885488800394122\n",
            "train loss:0.05076069278855819\n",
            "train loss:0.10548612927535045\n",
            "train loss:0.0984150355638998\n",
            "train loss:0.06784443047746463\n",
            "train loss:0.06903249733284961\n",
            "train loss:0.048419900717500894\n",
            "train loss:0.07923419711268977\n",
            "train loss:0.05296472651386277\n",
            "train loss:0.0804519472768789\n",
            "train loss:0.07196625599781309\n",
            "train loss:0.03424182233869817\n",
            "train loss:0.02635931680584824\n",
            "train loss:0.09659890262957575\n",
            "train loss:0.061010356208923436\n",
            "train loss:0.09606700855771656\n",
            "train loss:0.06080925052335703\n",
            "train loss:0.047197116043964024\n",
            "train loss:0.021636679369927272\n",
            "train loss:0.10843590692379543\n",
            "train loss:0.030434225149072937\n",
            "train loss:0.0775130466540312\n",
            "train loss:0.04707015409575102\n",
            "train loss:0.054526670674590785\n",
            "train loss:0.04548281029971709\n",
            "train loss:0.17378245653939628\n",
            "train loss:0.08895507956443499\n",
            "train loss:0.10347896491361817\n",
            "train loss:0.0607934065961669\n",
            "train loss:0.0329712470258175\n",
            "train loss:0.09719034970185073\n",
            "train loss:0.1175716098786201\n",
            "train loss:0.05669968937734818\n",
            "train loss:0.060716729335904815\n",
            "=== epoch:14, train acc:0.972, test acc:0.952 ===\n",
            "train loss:0.10497629617426975\n",
            "train loss:0.04993372514205663\n",
            "train loss:0.05259854163792983\n",
            "train loss:0.07363751479460802\n",
            "train loss:0.12110690108974403\n",
            "train loss:0.10166524783771465\n",
            "train loss:0.028939685155183983\n",
            "train loss:0.06652326044322746\n",
            "train loss:0.08248976629275306\n",
            "train loss:0.060517970065270106\n",
            "train loss:0.051184096402634154\n",
            "train loss:0.067760214574554\n",
            "train loss:0.06398863745540954\n",
            "train loss:0.11291179670072356\n",
            "train loss:0.06935705649469517\n",
            "train loss:0.04521360627946572\n",
            "train loss:0.10389682454712881\n",
            "train loss:0.12391070792876141\n",
            "train loss:0.062496337504229754\n",
            "train loss:0.06596127412889688\n",
            "train loss:0.05983275815470454\n",
            "train loss:0.05052305509265886\n",
            "train loss:0.12367818805771412\n",
            "train loss:0.06433049633113368\n",
            "train loss:0.07345648659567526\n",
            "train loss:0.022824164528619274\n",
            "train loss:0.04162120293201768\n",
            "train loss:0.08720960724181047\n",
            "train loss:0.16013188786882274\n",
            "train loss:0.05533354456533618\n",
            "train loss:0.07803539526622273\n",
            "train loss:0.02922240311009297\n",
            "train loss:0.03650268275086829\n",
            "train loss:0.11961294524205587\n",
            "train loss:0.05373253531500264\n",
            "train loss:0.08977522483637572\n",
            "train loss:0.05938755868685284\n",
            "train loss:0.028588327256881492\n",
            "train loss:0.04566912702313531\n",
            "train loss:0.04099729223001365\n",
            "train loss:0.03353211487324334\n",
            "train loss:0.04930637640451411\n",
            "train loss:0.03411481884998977\n",
            "train loss:0.022961907263069316\n",
            "train loss:0.02716138305188517\n",
            "train loss:0.055772070502954955\n",
            "train loss:0.08752270261129448\n",
            "train loss:0.06910146789167242\n",
            "train loss:0.052224316490753944\n",
            "train loss:0.11968454249829964\n",
            "=== epoch:15, train acc:0.973, test acc:0.954 ===\n",
            "train loss:0.018040498271857643\n",
            "train loss:0.11031168236889083\n",
            "train loss:0.060192450829815\n",
            "train loss:0.13173229776729387\n",
            "train loss:0.03780994167407202\n",
            "train loss:0.07356777261801987\n",
            "train loss:0.05089801306080263\n",
            "train loss:0.08096156497009162\n",
            "train loss:0.041637005932325485\n",
            "train loss:0.06055575105750819\n",
            "train loss:0.11526103840744764\n",
            "train loss:0.04298048417011433\n",
            "train loss:0.0626118411924953\n",
            "train loss:0.07761583013908312\n",
            "train loss:0.07519732454544455\n",
            "train loss:0.04440779763239297\n",
            "train loss:0.023526970309444447\n",
            "train loss:0.04760823444482833\n",
            "train loss:0.03518543780657038\n",
            "train loss:0.09611037739374259\n",
            "train loss:0.03919358199117593\n",
            "train loss:0.12356004531751928\n",
            "train loss:0.08886119812698925\n",
            "train loss:0.06170783969796357\n",
            "train loss:0.0364661599847202\n",
            "train loss:0.022310478724313784\n",
            "train loss:0.0695195063236754\n",
            "train loss:0.04427819681199248\n",
            "train loss:0.039473972145989834\n",
            "train loss:0.038614256246510895\n",
            "train loss:0.0865468712154152\n",
            "train loss:0.09281110015037665\n",
            "train loss:0.030695437857388617\n",
            "train loss:0.0205645537022188\n",
            "train loss:0.05153285283237561\n",
            "train loss:0.019684999391259894\n",
            "train loss:0.02376460637639065\n",
            "train loss:0.04459802023156396\n",
            "train loss:0.050517414043346164\n",
            "train loss:0.07181053103895213\n",
            "train loss:0.03549111847455555\n",
            "train loss:0.05698941764366042\n",
            "train loss:0.0749187252984314\n",
            "train loss:0.125531048139073\n",
            "train loss:0.08853821830966278\n",
            "train loss:0.08476412137103902\n",
            "train loss:0.029797125990571077\n",
            "train loss:0.014887639891744471\n",
            "train loss:0.05164263577822092\n",
            "train loss:0.09781290988974839\n",
            "=== epoch:16, train acc:0.977, test acc:0.95 ===\n",
            "train loss:0.08848597054740077\n",
            "train loss:0.030730053297389093\n",
            "train loss:0.04327782825483281\n",
            "train loss:0.023309010952983947\n",
            "train loss:0.04179096095872824\n",
            "train loss:0.05477502278374421\n",
            "train loss:0.07584862856386278\n",
            "train loss:0.12503022216974247\n",
            "train loss:0.03991889258877781\n",
            "train loss:0.04183247763560296\n",
            "train loss:0.04667529342854729\n",
            "train loss:0.045182329928390635\n",
            "train loss:0.03623213055264109\n",
            "train loss:0.07731772959111462\n",
            "train loss:0.05924353097854122\n",
            "train loss:0.02555683229919546\n",
            "train loss:0.02672667203786629\n",
            "train loss:0.08310974810583971\n",
            "train loss:0.02826370951951058\n",
            "train loss:0.02910393837247362\n",
            "train loss:0.06541859773495821\n",
            "train loss:0.04978025532375956\n",
            "train loss:0.06943424506095647\n",
            "train loss:0.06627121942418951\n",
            "train loss:0.0365332088172676\n",
            "train loss:0.08569370774245742\n",
            "train loss:0.05882084921172228\n",
            "train loss:0.019677457873801964\n",
            "train loss:0.04132698333550037\n",
            "train loss:0.018187212053822153\n",
            "train loss:0.06903407444613023\n",
            "train loss:0.028356019406275565\n",
            "train loss:0.043481274872490074\n",
            "train loss:0.04914340381544684\n",
            "train loss:0.07682213698324905\n",
            "train loss:0.05775232962528405\n",
            "train loss:0.025276062165931833\n",
            "train loss:0.07391744435942951\n",
            "train loss:0.035968061166029144\n",
            "train loss:0.017009515041590807\n",
            "train loss:0.0930264405025056\n",
            "train loss:0.041753383363764274\n",
            "train loss:0.012120726857566044\n",
            "train loss:0.02817548095083795\n",
            "train loss:0.0328870682297108\n",
            "train loss:0.03358911624718947\n",
            "train loss:0.06001359841016107\n",
            "train loss:0.0656255512157185\n",
            "train loss:0.05894663209264015\n",
            "train loss:0.018862706652131654\n",
            "=== epoch:17, train acc:0.984, test acc:0.955 ===\n",
            "train loss:0.05217816728401881\n",
            "train loss:0.04591324664045738\n",
            "train loss:0.062132345893074745\n",
            "train loss:0.05592259896063925\n",
            "train loss:0.03676234574571333\n",
            "train loss:0.04236365265562344\n",
            "train loss:0.04215709073708611\n",
            "train loss:0.016948810055270817\n",
            "train loss:0.025081972848379284\n",
            "train loss:0.034896956293264156\n",
            "train loss:0.016753475583305793\n",
            "train loss:0.022173086340562212\n",
            "train loss:0.037174503165179555\n",
            "train loss:0.04336238823561\n",
            "train loss:0.08212440772919974\n",
            "train loss:0.024654109870794926\n",
            "train loss:0.04130642148201411\n",
            "train loss:0.07464233997484232\n",
            "train loss:0.03174610406792571\n",
            "train loss:0.05103100434452895\n",
            "train loss:0.04324952791457637\n",
            "train loss:0.06623370917819825\n",
            "train loss:0.058211936545972474\n",
            "train loss:0.013664430521074456\n",
            "train loss:0.02450221988919849\n",
            "train loss:0.0494038256887243\n",
            "train loss:0.013574076883564623\n",
            "train loss:0.01930184340484652\n",
            "train loss:0.020154279147281563\n",
            "train loss:0.039599563169355194\n",
            "train loss:0.04659376195432255\n",
            "train loss:0.1209492532834627\n",
            "train loss:0.07801441224173555\n",
            "train loss:0.03203770081640842\n",
            "train loss:0.016967796617112535\n",
            "train loss:0.013054341670890346\n",
            "train loss:0.05068696685096809\n",
            "train loss:0.04760609376127381\n",
            "train loss:0.013138039968345314\n",
            "train loss:0.014051973184065203\n",
            "train loss:0.043960093917282374\n",
            "train loss:0.040873106220626135\n",
            "train loss:0.022512890859304583\n",
            "train loss:0.04149785225100816\n",
            "train loss:0.059776020525520154\n",
            "train loss:0.042311195735676585\n",
            "train loss:0.031474151755159746\n",
            "train loss:0.03575979319467059\n",
            "train loss:0.03479076006324236\n",
            "train loss:0.06500035218946199\n",
            "=== epoch:18, train acc:0.981, test acc:0.947 ===\n",
            "train loss:0.03897151102064713\n",
            "train loss:0.022322088966502614\n",
            "train loss:0.03651016960659992\n",
            "train loss:0.016996637265875084\n",
            "train loss:0.0382007640634996\n",
            "train loss:0.08217821761149019\n",
            "train loss:0.03284593380765668\n",
            "train loss:0.023821689309557366\n",
            "train loss:0.034800405872151496\n",
            "train loss:0.02758775955091498\n",
            "train loss:0.03274491364909167\n",
            "train loss:0.0388145735704721\n",
            "train loss:0.02030404645987013\n",
            "train loss:0.02160605297119809\n",
            "train loss:0.051018224122951844\n",
            "train loss:0.03532913891406357\n",
            "train loss:0.023083949835069246\n",
            "train loss:0.012819816889086642\n",
            "train loss:0.011444877912548623\n",
            "train loss:0.016183599423912443\n",
            "train loss:0.034349807467483055\n",
            "train loss:0.08064330833266325\n",
            "train loss:0.013628537979638573\n",
            "train loss:0.1427556275429433\n",
            "train loss:0.03564354289468659\n",
            "train loss:0.021483439152264982\n",
            "train loss:0.026725047569894297\n",
            "train loss:0.06477697360644202\n",
            "train loss:0.01532350607304689\n",
            "train loss:0.03458585698532762\n",
            "train loss:0.036972410099255476\n",
            "train loss:0.03002137831728132\n",
            "train loss:0.021569429078714307\n",
            "train loss:0.015237431219116915\n",
            "train loss:0.011374354353388252\n",
            "train loss:0.038909716874568845\n",
            "train loss:0.027779434427044447\n",
            "train loss:0.0470353175021271\n",
            "train loss:0.04121376604937092\n",
            "train loss:0.05190100570230951\n",
            "train loss:0.04157768736245592\n",
            "train loss:0.02425611695040134\n",
            "train loss:0.033667283224995036\n",
            "train loss:0.025052905810209646\n",
            "train loss:0.04865418474982811\n",
            "train loss:0.04253145834138655\n",
            "train loss:0.02959154152339395\n",
            "train loss:0.024648112157464062\n",
            "train loss:0.09602320301572523\n",
            "train loss:0.025910072344138082\n",
            "=== epoch:19, train acc:0.984, test acc:0.953 ===\n",
            "train loss:0.016954609769034137\n",
            "train loss:0.07299444018563238\n",
            "train loss:0.051878943302239716\n",
            "train loss:0.05842561806428168\n",
            "train loss:0.0622943888625104\n",
            "train loss:0.05193075094466741\n",
            "train loss:0.012653482304255005\n",
            "train loss:0.03701899142640053\n",
            "train loss:0.017356984346680714\n",
            "train loss:0.025969315287787533\n",
            "train loss:0.03415611598780613\n",
            "train loss:0.025050392872332176\n",
            "train loss:0.03643915487794115\n",
            "train loss:0.034975713011110215\n",
            "train loss:0.018454258173553065\n",
            "train loss:0.03583344107624361\n",
            "train loss:0.01783650411373844\n",
            "train loss:0.015884083815172055\n",
            "train loss:0.038894118951017374\n",
            "train loss:0.0708149650811759\n",
            "train loss:0.0324810986898938\n",
            "train loss:0.033046806211147646\n",
            "train loss:0.04377720269278049\n",
            "train loss:0.02950866112220168\n",
            "train loss:0.04097752618867853\n",
            "train loss:0.03486700619762237\n",
            "train loss:0.0736633012582261\n",
            "train loss:0.014579925259611064\n",
            "train loss:0.009162747027975024\n",
            "train loss:0.06539016088822104\n",
            "train loss:0.020761623245890024\n",
            "train loss:0.008326094803711789\n",
            "train loss:0.015987674968561256\n",
            "train loss:0.05288584345983085\n",
            "train loss:0.03242175162652022\n",
            "train loss:0.041290178082626285\n",
            "train loss:0.031429447502700575\n",
            "train loss:0.023269276528362335\n",
            "train loss:0.023674849853944968\n",
            "train loss:0.03622460147560411\n",
            "train loss:0.020727125133068878\n",
            "train loss:0.05258694489890587\n",
            "train loss:0.019235213974564548\n",
            "train loss:0.011315049061164682\n",
            "train loss:0.03325253231906298\n",
            "train loss:0.03795914692146174\n",
            "train loss:0.02038129323975849\n",
            "train loss:0.016862051526953692\n",
            "train loss:0.03915990864287134\n",
            "train loss:0.01008053210069474\n",
            "=== epoch:20, train acc:0.994, test acc:0.959 ===\n",
            "train loss:0.015468812313220743\n",
            "train loss:0.014875725220901909\n",
            "train loss:0.029614728417269805\n",
            "train loss:0.012717774730001312\n",
            "train loss:0.013072544371452206\n",
            "train loss:0.013918295254849843\n",
            "train loss:0.011127488539217823\n",
            "train loss:0.023926668657626004\n",
            "train loss:0.03729513508960423\n",
            "train loss:0.010926495772160824\n",
            "train loss:0.01615742185348568\n",
            "train loss:0.012163874675200642\n",
            "train loss:0.02103679419104984\n",
            "train loss:0.038134150328357055\n",
            "train loss:0.025101409033790364\n",
            "train loss:0.013381837514541474\n",
            "train loss:0.04964910053423597\n",
            "train loss:0.01631885925795312\n",
            "train loss:0.01687805617680941\n",
            "train loss:0.012151269705458316\n",
            "train loss:0.05636425431114305\n",
            "train loss:0.014518926252981283\n",
            "train loss:0.01870514456979799\n",
            "train loss:0.0226968746615653\n",
            "train loss:0.034341488996983666\n",
            "train loss:0.04988728501933345\n",
            "train loss:0.022401859336548667\n",
            "train loss:0.05697535602743315\n",
            "train loss:0.02046285242756149\n",
            "train loss:0.016789495270775768\n",
            "train loss:0.010778082008064597\n",
            "train loss:0.014161868384121408\n",
            "train loss:0.010833605568730174\n",
            "train loss:0.021398909964776468\n",
            "train loss:0.030012821736541847\n",
            "train loss:0.05165068444537683\n",
            "train loss:0.03174174573788182\n",
            "train loss:0.054936713452231827\n",
            "train loss:0.030004776858809916\n",
            "train loss:0.025121198500311217\n",
            "train loss:0.016422995806459013\n",
            "train loss:0.017825651981324676\n",
            "train loss:0.011396825530021953\n",
            "train loss:0.01253089330451273\n",
            "train loss:0.040285467691314095\n",
            "train loss:0.024726874944095943\n",
            "train loss:0.02522202794957233\n",
            "train loss:0.017120821833512916\n",
            "train loss:0.0871328835270786\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.954\n",
            "Saved Network Parameters!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYMj_x3Vb4wc",
        "colab_type": "text"
      },
      "source": [
        "### 7.5.1 CNN 성능 향상 방법  \n",
        "CNN의 성능을 향상시키는 가장 직접적인 방법은 망의 크기를 늘리는 것입니다.  \n",
        "여기서 망의 크기를 늘린다는 것은 망의 layer수(깊이)만을 늘리는 것뿐만 아니라, 각 layer의 넓이를 늘리는 것을 늘리는 것을 의미합니다.  \n",
        "* 깊은 망의 부작용  \n",
        "망이 깊어지면서 성능이 좋아지지만 2가지 문제가 발생합니다.  \n",
        "    1. 자유파라미터의 수가 증가하게 됩니다.  \n",
        "    학습에 사용할 데이터 양이 적은경우 오버피팅에 빠질 가능성이 높아집니다.  \n",
        "    2. 연산량이 늘어납니다.  \n",
        "    3. 기울기 손실이 발생합니다.  \n",
        "    Sigmoid 대신 ReLU를 활성화함수로 이용하는 이유도 기울기 손실 문제를 해결하기 위해서 입니다. (Sigmoid함수 양 끝의 기울기$\\rightarrow$0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EubZY8pTaUai",
        "colab_type": "text"
      },
      "source": [
        "## 7.6 CNN 시각화하기  \n",
        "### 7.6.1 1번째 층의 가중치 시각화하기\n",
        "학습 전 필터는 무작위로 초기화되고 있어 흑백의 정도에 규칙성이 없습니다.  \n",
        "반면 학습을 마친 필터는 에지와 블롭을 가진 규칙을 띄는 필터로 바뀌었습니다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlpAQtOAa0MS",
        "colab_type": "code",
        "outputId": "b9391e5a-5611-47c0-f4d6-95a0c91c63f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "source": [
        "def filter_show(filters, nx=8, margin=3, scale=10):\n",
        "    FN, C, FH, FW = filters.shape\n",
        "    ny = int(np.ceil(FN / nx))\n",
        "\n",
        "    fig = plt.figure()\n",
        "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "    for i in range(FN):\n",
        "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
        "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "network = SimpleConvNet()\n",
        "# 무작위(랜덤) 초기화 후의 가중치\n",
        "filter_show(network.params['W1'])\n",
        "\n",
        "# 학습된 가중치\n",
        "network.load_params(\"params.pkl\")\n",
        "filter_show(network.params['W1'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAEgCAYAAADMo8jPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 30 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAEgCAYAAADMo8jPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 30 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sq0zhlob18v",
        "colab_type": "text"
      },
      "source": [
        "### 7.6.2 층 깊이에 따른 추출 정보 변화\n",
        "1번째 층의 합성곱 계층에서 에지나 블롭 등의 저수준 정보를 추출할 수 있음을 확인했습니다.  \n",
        "겹겹이 쌓인 CNN의 각 계층이 깊어질수록 추출되는 정보는 더 추상화됩니다.  \n",
        "\n",
        "Ex)일반 사물 인식을 수행한 8층의 CNN네트워크  \n",
        "1번째 층에서은 에지나 블롭을, 3번째 층에서 텍스처, 5번째 층에서 사물의 일부,  \n",
        "마지막 완전연결 계층에서는 사물의 클래스(개, 자동차 등)에 뉴런이 반응합니다.  \n",
        "\n",
        "층이 깊어지면서 신경망학습을 통해 사물의 '의미'를 이해할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woXg0jShj_S8",
        "colab_type": "text"
      },
      "source": [
        "## 7.7 대표적인 CNN  \n",
        "![](https://tensorflowkorea.files.wordpress.com/2016/09/ilsvr2016-2.png)\n",
        "### 7.7.1 LeNet\n",
        "* 20년 전에 제안된 '첫 CNN'  \n",
        "* 합성곱 계층과 풀링계층을 반복하고 완전연결 계층을 거치면서 결과 출력    \n",
        "* 현재의 CNN과 차이점\n",
        "  1. 활성화함수 : 시그모이드 / 현재는 주로 ReLU  \n",
        "  2. 풀링계층 : 단순히 '원소를 줄이기'만 하는 서브샘플링 계층 / 현재는 주로 최대 풀링\n",
        "\n",
        "<img width=\"514\" alt=\"LeNet\" src=\"https://user-images.githubusercontent.com/53211502/68599454-eee08400-04e3-11ea-9204-ad05c33336ab.PNG\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk-6WM9Fj-2R",
        "colab_type": "text"
      },
      "source": [
        "### 7.7.2  AlexNet  \n",
        "<img width=\"536\" alt=\"AlexNet\" src=\"https://user-images.githubusercontent.com/53211502/68599686-5991bf80-04e4-11ea-9e36-19cdd7ac2968.PNG\">\n",
        "\n",
        "* 2012년에 발표\n",
        "* LeNet의 특이사항.  \n",
        "    1. 활성화 함수로 주로 ReLU를 이용\n",
        "    2. LRN이라는 국소적 정규화를 실시하는 계층을 이용 \n",
        "    3. 드롭아웃 사용(128배치 사이즈)\n",
        "    4. SGD 모멘텀 사용\n",
        "    5. CONV1 층을 보시면 두 개로 나눠져 있는데  \n",
        "    3GB 메모리의 GPU 2개를 사용하여 연결한 네트워크 입니다.\n",
        "\n",
        "<img width=\"81\" alt=\"AlexNet structure\" src=\"https://user-images.githubusercontent.com/53211502/68599943-e3da2380-04e4-11ea-946d-238edf8c63eb.PNG\">  \n",
        "\n",
        "Input : 227x227x3 images  \n",
        "첫 번째 층(CONV1) : 96x11x11 (stride=4) 필터 이용  \n",
        "$\\rightarrow$ Output volume : ((227-11)/4+1=55) 55x55x96(필터 수) / parmameters : (11x11x3(input의 깊이))x96 = 35k  \n",
        "두 번째 층(POOL1) : 3x3 (stride=2) 필터 사용  \n",
        "$\\rightarrow$ Output volume : ((55-3)/2+1=27) 27x27x96(pooling은 깊이 보존) / parmameters : 0 (pooling은 최댓값 혹은 평균값을 계산) 을 할 수 있습니다.\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3BJ-MDH_Cx7",
        "colab_type": "text"
      },
      "source": [
        "### 7.7.3 VGG  \n",
        "<img width=\"140\" alt=\"VGG\" src=\"https://user-images.githubusercontent.com/53211502/68602134-243ba080-04e9-11ea-9caa-e3e220ccd53f.PNG\">\n",
        "\n",
        "* 옥스포드에서 개발\n",
        "* AlexNet과 유사 (LRN 사용 X)\n",
        "* 한 층 더 깊어진 네트워크(19layers) & 더 작아진 필터 이용(CONV 3x3)\n",
        "\n",
        "![](https://i.stack.imgur.com/5tp2P.png)\n",
        "\n",
        "3x3 3개의 필터 = 7x7 1개의 필터\n",
        "$\\rightarrow$ 더 적은 parameters (3x3x3<7x7)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgrX_q2IeJJz",
        "colab_type": "text"
      },
      "source": [
        "### 7.7.4 ResNet  \n",
        "* 깊은 망의 문제점  \n",
        "ResNet팀에서 20-layer, 56-layer에 대하여 비교 테스트를 진행하였습니다.  \n",
        "그 결과로는 56-layer의 오차가 더 높았습니다.  \n",
        "그 원인으로는 기울기 손실이 발생하였기 때문입니다.  \n",
        "\n",
        "<img width=\"398\" alt=\"ResNet2\" src=\"https://user-images.githubusercontent.com/53211502/68604068-fb1d0f00-04ec-11ea-8b37-5f1bb169869e.PNG\">\n",
        "\n",
        "* skip connection  \n",
        "기존의 신경망의 학습 목적이 입력(x)을 타겟값(y)으로 모델링하는 함수  \n",
        "H(x)를 찾는 것이라고 한다면, H(x)−y를 최소화 하는 방향으로 학습을 진행합니다.   \n",
        "이 때 x와 짝지어진 y는 사실 x를 대변하는 것으로, 특히 이미지 분류 문제에서는 네트워크의 입 출력을 의미상 같게끔 해야합니다.  \n",
        "그래서 ResNet에서는 관점을 바꿔 네트워크가 H(x)−x를 얻는 것으로 목표를 수정하였습니다.  \n",
        "입력과 출력의 잔차를 F(x)=H(x)−x라고 정의하고 네트워크는 이 F(x)를 찾는 것입니다.  \n",
        "이렇게 잔차를 학습하는 것을 Residual learning, Residual mapping이라고 합니다.  \n",
        "이렇게 네트워크의 입력과 출력이 더해진 것을 다음 레이러의 입력으로 사용하는 것을 스킵연결(skip connection) 이라고 한다.  \n",
        "기존의 신경망은 H(x)가 어떻게든 정답과 같게 만드는 것이 목적이었다면, 이제 입력과 출력 사이의 잔차를 학습하는 것, 즉 최적의 경우  F(x)=0 이 되어야하므로 학습의 목표가 이미 정해져 있기 때문에 학습 속도가 빨라질 것이고, 네트워크가 잔차를 학습하고 나면, 입력값의 작은 변화에도 민감하게 반응 할 것이다라는 것이 ResNet의 가설입니다."
      ]
    }
  ]
}