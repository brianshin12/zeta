{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 학습  \n",
    "\n",
    "* 학습  \n",
    "    훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것을 뜻합니다.  \n",
    "* 손실 함수\n",
    "    신경망이 학습할 수 있도록 해주는 지표  \n",
    "    손실 함수의 결괏값을 가장 작게 만드는 가중치 매개변수를 찾는 것이 목표  \n",
    "\n",
    "## 4.1 데이터에서 학습한다!  \n",
    "\n",
    "* 신경망의 특징: 데이터를 보고 학습, 가중치 매개변수의 값을 데이터를 보고 자동으로 결정  \n",
    "\n",
    "### 4.1.1 데이터 주도 학습  \n",
    "\n",
    "* 기계학습의 핵심: 데이터, 사람의 개입을 최소화 수집한 데이터로부터 패턴 발견  \n",
    "    Ex)'5'를 분류하는 프로그램\n",
    "    * 알고리즘을 밑바닥부터 설계하는 대신, 주어진 데이터를 잘 활용  \n",
    "    신경망: 이미지에 포함된 중요한 특징까지 있는 그대로 학습.(종단간 기계학습)  \n",
    "    \n",
    "### 4.1.2 훈련데이터와 시험데이터  \n",
    "\n",
    "* 훈련데이터&시험데이터  \n",
    "    * 훈련데이터: 훈련데이터만을 이용하여 최적의 매개변수를 찾는다.\n",
    "    * 시험데이터: 시험데이터를 사용하여 앞서 훈련한 모델의 실력을 평가, 범용능력을 평가  \n",
    "    * 오버피팅: 한 데이터셋에만 지나치게 최적화된 상태\n",
    "    \n",
    "## 4.2 손실 함수  \n",
    "\n",
    "* 신경망 학습에서는 현재의 상태를 '하나의 지표'(손실 함수)로 표현  \n",
    "    그 지표(손실 함수)를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색  \n",
    "    일반적으로 편균 제곱 오차와 교차 엔트로피 오차 사용  \n",
    "    \n",
    "### 4.2.1 평균 제곱 오차  \n",
    "$$E = {\\sum_{}^{} ({y_k-t_k})^2\\over2}$$  \n",
    "* y: 신경망이 추정한 값\n",
    "* t: 정답 레이블\n",
    "* k: 데이터 차원 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def mean_squared_error(y,t):\n",
    "    return 0.5*np.sum((y-t)**2)\n",
    "\n",
    "t=[0,0,1,0,0,0,0,0,0,0] #정답은 2\n",
    "y=[0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] #'2'일 확률이 가장 높다고 추정 (소프트맥스 함수의 출력)\n",
    "mean_squared_error(np.array(y), np.array(t)) #아래의 예시보다 오차가 더 작다, 정답에 더 가까울 것으로 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=[0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0] #'7'일 확률이 가장 높다고 추정 \n",
    "mean_squared_error(np.array(z), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 교차 엔트로피 오차  \n",
    "$$E = {-\\sum_{}^{} t_k ln y_k}$$  \n",
    "* y:신경망의 출력  \n",
    "* t:정답 레이블  \n",
    "    t: 정답에 해당하는 인덱스의 원소만 1이고 나머지는 0 (원-핫 인코딩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta=1e-7 #마이너스 무한대 방지\n",
    "    return -np.sum(t*np.log(y+delta))\n",
    "\n",
    "t=[0,0,1,0,0,0,0,0,0,0]\n",
    "y=[0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=[0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 미니배치 학습  \n",
    "* 훈련 데이터 모두에 대한 손실함수의 합\n",
    "$$E = {-{1 \\over N} \\times \\sum_{n}^{} \\sum_{k}^{} t_{nk} ln y_{nk}}$$  \n",
    "\n",
    "* 많은 데이터를 대상으로 일일이 손실 함수를 계산하는 것은 현실적이지 않다.  \n",
    "    데이터를 일부 추려 전체의 '근사치'로 이용 → 미니배치 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "#mnist dataset을 읽어오는 코드\n",
    "import sys, os \n",
    "os.chdir(\"C:\\\\Users\\\\신현승\\\\deeplearning\\\\dataset\")\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "(x_train, t_train), (x_test, t_test)=\\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape) #훈련 데이터는 60000개, 입력 데이터는 784열\n",
    "print(t_train.shape) #정답 레이블은 10줄 따리 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14101, 32250, 19699,  5687, 13309, 18884, 46450, 53078,  3821,\n",
       "       49494])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#무작위로 10장만 빼내는 코드\n",
    "train_size=x_train.shape[0]\n",
    "batch_size=10\n",
    "batch_mask=np.random.choice(train_size, batch_size)\n",
    "x_batch=x_train[batch_mask]\n",
    "t_batch=t_train[batch_mask]\n",
    "\n",
    "np.random.choice(60000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 (배치용) 교차 엔트로피 오차 구현하기  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim==1:\n",
    "        t=t.reshape(1, t.size)\n",
    "        y=y.reshape(1, y.size)\n",
    "        \n",
    "    bath_size=y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size),t]+1e-7))/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* y: 신경망 출력  \n",
    "    y가 1차원이라면(데이터 하나당 교차 엔트로피 오차를 구하는 경우) reshape 함수로 데이터 형상을 바꿔줍니다.  \n",
    "    배치의 크기로 나눠 정규화하고 이미지 1장당 평균의 교차 엔트로피 오차를 계산합니다.\n",
    "* t: 정답 레이블"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 왜 손실 함수를 설정하는가?  \n",
    "\n",
    "* 궁극적인 목표: 높은 '정확도'를 끌어내는 매개변수 값을 찾는 것.  \n",
    "* 신경망 학습에서의 '미분'의 역할에 주목  \n",
    "    최적의 매개변수를 탐색 → 손실 함수의 값을 가능한 작게하는 매개변수 값을 찾음  \n",
    "    →매개병수의 미분 값을 단서로 매개변수의 값을 갱신하는 과정 반복\n",
    "* 미분 값이 음수이면 가중치 매개변수를 양의 방향으로 변화시켜 손실 함수의 값을 줄임  \n",
    "    미분 값이 양수이면 가중치 매개변수를 음의 방향으로 변화시켜 손실 함수의 값을 줄임  \n",
    "* 미분 값이 0 이면 가중치 매개변수의 갱신 멈춤 → '정확도'가 아닌 '손실 함수의 값'을 지표로 삼는 이유  \n",
    "    Ex1) 100장의 훈련 데이터 중 32장을 올바로 인식  \n",
    "    이때 정확도는 32%이지만 매개변수를 약간만 조정해서는 정확도가 개선되지 않고 개선된다 하더라도 불연속적인 값으로 바뀜  \n",
    "    '계단 함수'를 활성화 함수로 사용하지 않는 이유와도 일치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
